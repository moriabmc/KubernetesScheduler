{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fa8ac6f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import csv\n",
    "import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import pandas as pd\n",
    "from pandas import concat\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, LSTM\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler, LabelEncoder\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "def calc_avg(lst):\n",
    "    total = 0\n",
    "    for element in lst:\n",
    "        total += element\n",
    "    if len(lst) < 1:\n",
    "        return 0\n",
    "    else:\n",
    "        return total/len(lst)\n",
    "    \n",
    "def read_data(filename):\n",
    "    nprocs = []\n",
    "    runtime = []\n",
    "    total_jobs = []\n",
    "    submit_time = []\n",
    "    \n",
    "    core_count = []\n",
    "    r = []\n",
    "    \n",
    "    \n",
    "    with open(filename) as file:\n",
    "        tsv_file = csv.reader(file, delimiter=\"\\t\")\n",
    "        field_count = 0\n",
    "        while int(field_count) < 20:\n",
    "            field_count = int(len(next(tsv_file)))\n",
    "\n",
    "        last_time = -1\n",
    "        last_hour = -1\n",
    "        job_count = 0\n",
    "        last_submitted = -1\n",
    "        i = 0\n",
    "        for line in tsv_file:\n",
    "            i += 1\n",
    "            #if i > 30000:\n",
    "                # break\n",
    "            if (float(line.__getitem__(3)) > -0.5) and (float(line.__getitem__(4)) > -0.5):\n",
    "                submitted = int(line.__getitem__(1))\n",
    "                dt = datetime.datetime.fromtimestamp(submitted)\n",
    "                time_hour = dt.hour\n",
    "\n",
    "                if last_hour == -1:\n",
    "                    last_hour = time_hour\n",
    "                    last_time = dt\n",
    "                \n",
    "                if last_hour != time_hour:\n",
    "                    runtime.append(calc_avg(r))\n",
    "                    nprocs.append(calc_avg(core_count))\n",
    "                    total_jobs.append(job_count)\n",
    "                    submit_time.append(dt.replace(minute=0, second=0, microsecond=0))\n",
    "                    last_time = dt\n",
    "                    job_count = 0\n",
    "                    core_count.clear\n",
    "                    r.clear\n",
    "                    last_submitted = -1\n",
    "                \n",
    "                core_count.append(float(line.__getitem__(4))) # number of allocated processors\n",
    "                r.append(float(line.__getitem__(3))) # runtime of the job\n",
    "                job_count += 1\n",
    "                last_submitted = submitted\n",
    "                last_hour = time_hour\n",
    "    return submit_time, runtime, nprocs, total_jobs\n",
    "\n",
    "def read_dataframe():\n",
    "    submit_time, runtime, nprocs, total_jobs = read_data('anon_jobs_Sharc.gwf') #SharcNet\n",
    "    df = pd.DataFrame(list(zip(submit_time, runtime, nprocs, total_jobs)), columns=['ds', 'RunTime', 'NProcs', 'TotalJobs'])\n",
    "    df.to_pickle('job_traces')\n",
    "    return df\n",
    "\n",
    "def load_dataframe():\n",
    "    return pd.read_pickle('total_jobs_dataframe')\n",
    "\n",
    "def series_to_supervised(data, n_in=1, n_out=1, dropnan=True):\n",
    "    n_vars = 1 if type(data) is list else data.shape[1]\n",
    "    df = DataFrame(data)\n",
    "    cols, names = list(), list()\n",
    "    # input sequence (t-n, ... t-1)\n",
    "    for i in range(n_in, 0, -1):\n",
    "        cols.append(df.shift(i))\n",
    "        names += [('var%d(t-%d)' % (j+1, i)) for j in range(n_vars)]\n",
    "    # forecast sequence (t, t+1, ... t+n)\n",
    "    for i in range(0, n_out):\n",
    "        cols.append(df.shift(-i))\n",
    "        if i == 0:\n",
    "            names += [('var%d(t)' % (j+1)) for j in range(n_vars)]\n",
    "        else:\n",
    "            names += [('var%d(t+%d)' % (j+1, i)) for j in range(n_vars)]\n",
    "    # put it all together\n",
    "    agg = concat(cols, axis=1)\n",
    "    agg.columns = names\n",
    "    # drop rows with NaN values\n",
    "    if dropnan:\n",
    "        agg.dropna(inplace=True)\n",
    "    return agg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "98d074f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                      ds       RunTime     NProcs  TotalJobs\n",
      "0    2006-01-24 17:00:00     13.600000   5.400000          5\n",
      "1    2006-01-24 18:00:00     77.000000  19.500000          9\n",
      "2    2006-01-24 19:00:00    148.625000  27.062500          2\n",
      "3    2006-01-24 20:00:00    119.105263  48.947368         22\n",
      "4    2006-01-24 21:00:00    171.384615  73.948718          1\n",
      "...                  ...           ...        ...        ...\n",
      "7071 2007-01-15 21:00:00  31829.883378   3.013483        254\n",
      "7072 2007-01-15 22:00:00  31828.379609   3.013501         58\n",
      "7073 2007-01-15 23:00:00  31826.919224   3.013525         57\n",
      "7074 2007-01-16 00:00:00  31825.394948   3.013849         59\n",
      "7075 2007-01-16 01:00:00  31825.058342   3.013981         13\n",
      "\n",
      "[7076 rows x 4 columns]\n"
     ]
    }
   ],
   "source": [
    "dataframe = read_dataframe()\n",
    "dataframe.set_index('ds')\n",
    "values = dataset.values\n",
    "print(dataframe)\n",
    "# integer encode direction\n",
    "encoder = LabelEncoder()\n",
    "values[:,4] = encoder.fit_transform(values[:,4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5185f246",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b9820583",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 5]\n",
      " [ 9]\n",
      " [ 2]\n",
      " ...\n",
      " [57]\n",
      " [59]\n",
      " [13]]\n"
     ]
    }
   ],
   "source": [
    "tf.random.set_seed(7)\n",
    "dataset = dataframe[['TotalJobs']].values\n",
    "print(dataset)\n",
    "dataset = dataset.astype('float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "dc369996",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4740 2336\n"
     ]
    }
   ],
   "source": [
    "# normalize the dataset\n",
    "scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "dataset = scaler.fit_transform(dataset)\n",
    "\n",
    "# train test split\n",
    "train_size = int(len(dataset) * 0.67)\n",
    "test_size = len(dataset) - train_size\n",
    "train, test = dataset[0:train_size,:], dataset[train_size:len(dataset),:]\n",
    "print(len(train), len(test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "e70239c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dataset(dataset, look_back=1):\n",
    "    dataX, dataY = [], []\n",
    "    for i in range(len(dataset)-look_back-1):\n",
    "        a = dataset[i:(i+look_back), 0]\n",
    "        dataX.append(a)\n",
    "        dataY.append(dataset[i + look_back, 0])\n",
    "    return np.array(dataX), np.array(dataY)\n",
    "\n",
    "# reshape into X=t and Y=t+1\n",
    "look_back = 1\n",
    "trainX, trainY = create_dataset(train, look_back)\n",
    "testX, testY = create_dataset(test, look_back)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "012a2ead",
   "metadata": {},
   "outputs": [],
   "source": [
    "# reshape input to be [samples, time steps, features]\n",
    "trainX = np.reshape(trainX, (trainX.shape[0], 1, trainX.shape[1]))\n",
    "testX = np.reshape(testX, (testX.shape[0], 1, testX.shape[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a094168",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "4738/4738 - 5s - loss: 9.1769e-04 - 5s/epoch - 1ms/step\n",
      "Epoch 2/100\n",
      "4738/4738 - 4s - loss: 9.1173e-04 - 4s/epoch - 891us/step\n",
      "Epoch 3/100\n",
      "4738/4738 - 4s - loss: 9.1304e-04 - 4s/epoch - 864us/step\n",
      "Epoch 4/100\n"
     ]
    }
   ],
   "source": [
    "# create and fit the LSTM network\n",
    "model = Sequential()\n",
    "model.add(LSTM(4, input_shape=(1, look_back)))\n",
    "model.add(Dense(1))\n",
    "model.compile(loss='mean_squared_error', optimizer='adam')\n",
    "model.fit(trainX, trainY, epochs=100, batch_size=1, verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "680cbab3",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(trainY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e29bd24",
   "metadata": {},
   "outputs": [],
   "source": [
    "# make predictions\n",
    "trainPredict = model.predict(trainX)\n",
    "testPredict = model.predict(testX)\n",
    "# invert predictions\n",
    "trainPredict = scaler.inverse_transform(trainPredict)\n",
    "trainY = scaler.inverse_transform(trainY)\n",
    "testPredict = scaler.inverse_transform(testPredict)\n",
    "testY = scaler.inverse_transform(testY)\n",
    "# calculate root mean squared error\n",
    "trainScore = np.sqrt(mean_squared_error(trainY[0], trainPredict[:,0]))\n",
    "print('Train Score: %.2f RMSE' % (trainScore))\n",
    "testScore = np.sqrt(mean_squared_error(testY[0], testPredict[:,0]))\n",
    "print('Test Score: %.2f RMSE' % (testScore))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "135a5572",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(range(0,24), testPredict[:24])\n",
    "plt.plot(range(0,24), testY[0][:24])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96dc73c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# shift train predictions for plotting\n",
    "trainPredictPlot = np.empty_like(dataset)\n",
    "trainPredictPlot[:, :] = np.nan\n",
    "trainPredictPlot[look_back:len(trainPredict)+look_back, :] = trainPredict\n",
    "# shift test predictions for plotting\n",
    "testPredictPlot = np.empty_like(dataset)\n",
    "testPredictPlot[:, :] = np.nan\n",
    "testPredictPlot[len(trainPredict)+(look_back*2)+1:len(dataset)-1, :] = testPredict\n",
    "# plot baseline and predictions\n",
    "plt.plot(scaler.inverse_transform(dataset))\n",
    "plt.plot(trainPredictPlot)\n",
    "plt.plot(testPredictPlot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abf9a484",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
